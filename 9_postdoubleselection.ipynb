{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 8 - Post-double Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Frisch-Waugh Theorem](#Frisch-Waugh-Theorem)\n",
    "- [Post-double selection](#Post-double-selection)\n",
    "- [Double/debiased machine learning](#Double/debiased-machine-learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from numpy.linalg import inv\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib for graphs\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "# Set global parameters\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "plt.rcParams['lines.linewidth'] = 3\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['figure.titlesize'] = 20\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['legend.fontsize'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frisch-Waugh theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the data $D = \\{ x_i, y_i, z_i \\}_{i=1}^\\infty$ with DGP:\n",
    "\n",
    "$$\n",
    "y_i = \\alpha x_i + \\beta z_i + \\varepsilon_i\n",
    "$$\n",
    "\n",
    ". The following estimators of $\\alpha$ are numerically equivalent (if $[x, z]$ has full rank):\n",
    "\n",
    "- OLS: $\\hat{\\alpha}$ from regressing $y$ on $x, z$\n",
    "- Partialling out: $\\tilde{\\alpha}$ from regressing $y$ on $\\tilde{x}$\n",
    "- \"Double\" partialling out: $\\bar{\\alpha}$ from regressing $\\tilde{y}$ on $\\tilde{x}$\n",
    "\n",
    "where the operation of passing to $y, x$ to $\\tilde{y}, \\tilde{x}$ is called *projection  out $z$*, e.g. $\\tilde{x}$ are the residuals from regressing $x$ on $z$.\n",
    "\n",
    "$$\n",
    "\\tilde{x} = (I - z (z' z)^{-1} z' ) x = (I-P) x = M x\n",
    "$$\n",
    "\n",
    "I.e we have done the following:\n",
    " 1. regress $x$ on $z$\n",
    " 2. compute $\\hat x$\n",
    " 3. compute the residuals $\\tilde x = x - \\hat x$\n",
    " \n",
    "We now explore the theorem through simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# Init\n",
    "N = 1000\n",
    "a = 2\n",
    "b = -1\n",
    "\n",
    "# Generate data\n",
    "x = np.random.uniform(0,1,N).reshape(-1,1)\n",
    "z = np.random.uniform(0,1,N).reshape(-1,1)\n",
    "e = np.random.normal(0,1,N).reshape(-1,1)\n",
    "y = a*x + b*z + e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha OLS: 2.0928 (true=2)\n"
     ]
    }
   ],
   "source": [
    "# Estimate alpha by OLS\n",
    "xz = np.concatenate([x,z], axis=1)\n",
    "ols_coeff = inv(xz.T @ xz) @ xz.T @ y\n",
    "alpha_ols = ols_coeff[0][0]\n",
    "\n",
    "print('alpha OLS: %.4f (true=%1.0f)' % (alpha_ols, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha partialling out: 2.0928 (true=2)\n"
     ]
    }
   ],
   "source": [
    "# Partialling out\n",
    "x_tilde = (np.eye(N) - z @ inv(z.T @ z) @ z.T ) @ x\n",
    "alpha_po = inv(x_tilde.T @ x_tilde) @ x_tilde.T @ y\n",
    "\n",
    "print('alpha partialling out: %.4f (true=%1.0f)' % (alpha_po, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha double partialling out: 2.0928 (true=2)\n"
     ]
    }
   ],
   "source": [
    "# \"Double\" partialling out\n",
    "y_tilde = (np.eye(N) - z @ inv(z.T @ z) @ z.T ) @ y\n",
    "alpha_po2 = inv(x_tilde.T @ x_tilde) @ x_tilde.T @ y_tilde\n",
    "\n",
    "print('alpha double partialling out: %.4f (true=%1.0f)' % (alpha_po2, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Omitted Variable Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider two separate statistical models. Assume the following **long regression** of interest:\n",
    "\n",
    "$$\n",
    "y_i = x_i' \\alpha_0+ z_i' \\beta_0 + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "Define the corresponding **short regression** as\n",
    "\n",
    "$$\n",
    "  y_i = x_i' \\alpha_0 + v_i \\quad \\text{ with } \\quad v_i = z_i' \\beta_0 + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "**OVB Theorem**:\n",
    "Suppose that the DGP for the long regression corresponds to $\\alpha_0$, $\\beta_0$. Suppose further that $\\mathbb E[x_i] = 0$, $\\mathbb E[z_i] = 0$, $\\mathbb E[\\varepsilon_i |x_i,z_i] = 0$. Then, unless $\\beta_0 = 0$ or $z_i$ is orthogonal to $x_i$, the (sole) stochastic regressor $x_i$ is correlated with the error term in the short regression which implies that the OLS estimator of the short regression is inconsistent for $\\alpha_0$ due to the omitted variable bias. In particular, one can show that the plim of the OLS estimator of $\\hat{\\alpha}_{SHORT}$ from the short regression is\n",
    "$$\n",
    "\\hat{\\alpha}_{SHORT} \\overset{p}{\\to} \\frac{Cov(y_i, x_i)}{Var(x_i)} = \\alpha_0 + \\beta_0 \\frac{Cov(z_i, x_i)}{Var(x_i)}\n",
    "$$\n",
    "\n",
    "Consider data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& y_i = x_i' \\alpha_0  + z_i' \\beta_0 + \\varepsilon_i \\\\\n",
    "& x_i = z_i' \\gamma_0 + u_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let's investigate the Omitteb Variable Bias by simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# Init\n",
    "N = 1000\n",
    "a = 2\n",
    "b = -1\n",
    "c = 3\n",
    "\n",
    "# Generate data\n",
    "z = np.random.normal(0,1,N).reshape(-1,1)\n",
    "u = np.random.normal(0,1,N).reshape(-1,1)\n",
    "x = c*z + u\n",
    "e = np.random.normal(0,1,N).reshape(-1,1)\n",
    "y = a*x + b*z + e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha OLS: 1.6866 (true=2)\n"
     ]
    }
   ],
   "source": [
    "# Estimate alpha by OLS\n",
    "ols_coeff = inv(x.T @ x) @ x.T @ y\n",
    "alpha_ols = ols_coeff[0][0]\n",
    "\n",
    "print('alpha OLS: %.4f (true=%1.0f)' % (alpha_ols, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case the expected bias is:\n",
    "\n",
    "$$\n",
    "Bias = \\beta_0 \\frac{Cov(z_i, x_i)}{Var(x_i)} = \\\\\n",
    "= \\beta_0 \\frac{Cov(z_i' \\gamma_0 + u_i, x_i)}{Var(z_i' \\gamma_0 + u_i)} = \\\\\n",
    "= \\beta_0 \\frac{\\gamma_0 Var(z_i)}{\\gamma_0^2 Var(z_i) + Var(u_i)} = \\\\\n",
    "= b \\frac{c}{c^2 + 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias: -0.3134 (expected=-0.3000)\n"
     ]
    }
   ],
   "source": [
    "# Expected bias\n",
    "bias = alpha_ols - a\n",
    "exp_bias = b * c / (c**2 + 1)\n",
    "\n",
    "print('Bias: %.4f (expected=%.4f)' % (bias, exp_bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-test bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& y_i = x_i' \\alpha_0  + z_i' \\beta_0 + \\varepsilon_i \\\\\n",
    "& x_i = z_i' \\gamma_0 + u_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where $x_i$ is the variable of interest (we want to make inference on $\\alpha_0$) and $z_i$ is a high dimensional set of control variables. \n",
    "\n",
    "From now on, we will work under the following assumptions:\n",
    "\n",
    "- $\\dim(x_i)=1$ for all $n$\n",
    "- $\\beta_0$ uniformely bounded in $n$\n",
    "- Strict exogeneity: $\\mathbb E[\\varepsilon_i | x_i, z_i] = 0$ and $\\mathbb E[u_i | z_i] = 0$\n",
    "- $\\beta_0$ and $\\gamma_0$ have dimension (and hence value) that depend on $n$\n",
    "\n",
    "Pre-Testing procedure:\n",
    "\n",
    "1. Regress $y_i$ on $x_i$ and $z_i$\n",
    "2. For each $j = 1, ..., p = \\dim(z_i)$ calculate a test statistic $t_j$\n",
    "3. Let $\\hat{T} = \\{ j: |t_j| > C > 0 \\}$ for some constant $C$ (set of statistically significant coefficients).\n",
    "4. Re-run the new \"model\" using $(x_i, z_{\\hat{T},i})$ (i.e. using the selected covariates with statistically significant coefficients).\n",
    "5. Perform statistical inference (i.e. confidence intervals and hypothesis tests) as if no model selection had been done.\n",
    "\n",
    "Pre-testing leads to incorrect inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-testing code\n",
    "def pre_testing(a, b, c, N, simulations):\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # Init\n",
    "    alpha = {'Long': np.zeros((simulations,1)),\n",
    "            'Short': np.zeros((simulations,1)),\n",
    "            'Pre-test': np.zeros((simulations,1))}\n",
    "\n",
    "    # Loop over simulations\n",
    "    for i in range(simulations):\n",
    "        \n",
    "        # Generate data\n",
    "        z = np.random.normal(0,1,N).reshape(-1,1)\n",
    "        u = np.random.normal(0,1,N).reshape(-1,1)\n",
    "        x = c*z + u\n",
    "        e = np.random.normal(0,1,N).reshape(-1,1)\n",
    "        y = a*x + b*z + e\n",
    "        \n",
    "        # Compute coefficients\n",
    "        xz = np.concatenate([x,z], axis=1)\n",
    "        alpha['Long'][i] = (inv(xz.T @ xz) @ xz.T @ y)[0][0]\n",
    "        alpha['Short'][i] = inv(x.T @ x) @ x.T @ y\n",
    "        \n",
    "        # Compute significance test on beta_hat\n",
    "        beta_hat = (inv(xz.T @ xz) @ xz.T @ y)[1][0]\n",
    "        residuals = y - alpha['Long'][i]*x - beta_hat*y\n",
    "        sigma_hat = np.var(residuals)\n",
    "        beta_std = np.sqrt( inv(xz.T @ xz)[1,1] * sigma_hat )\n",
    "        t = beta_hat/beta_std\n",
    "        \n",
    "        # Select specification based on test\n",
    "        if abs(t)>1.96:\n",
    "            alpha['Pre-test'][i] = alpha['Long'][i]\n",
    "        else:\n",
    "            alpha['Pre-test'][i] = alpha['Short'][i]\n",
    "    \n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean alpha Long = 1.9994\n",
      "Mean alpha Short = 1.6995\n",
      "Mean alpha Pre-test = 1.9701\n"
     ]
    }
   ],
   "source": [
    "# Init\n",
    "simulations = 1000\n",
    "N = 1000\n",
    "a = 2\n",
    "b = -1\n",
    "c = 3\n",
    "\n",
    "# Get pre_test alpha\n",
    "alpha = pre_testing(a, b, c, N, simulations)\n",
    "\n",
    "for key, value in alpha.items():\n",
    "    print('Mean alpha %s = %.4f' % (key, np.mean(value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(16,6))\n",
    "\n",
    "# Plot distributions\n",
    "ax1.hist(alpha['Long'], bins=30); ax1.set_title('Long Regression')\n",
    "ax2.hist(alpha['Short'], bins=30); ax2.set_title('Short Regression')\n",
    "ax3.hist(alpha['Pre-test'], bins=30); ax3.set_title('Pre Testing')\n",
    "\n",
    "\n",
    "# All axes\n",
    "x_max = np.max([np.max(np.abs(x-a)) for x in alpha.values()])\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlim([a-x_max, a+x_max])\n",
    "    ax.axvline(a, c='r', ls='--')\n",
    "    ax.legend([r'$\\alpha_0=2$', r'$\\hat \\alpha$'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When is pre-testing a problem? When the magnitude of $\\alpha$ is big with respect to the sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show pre-testing with different beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it mean \"*big with respect to the sample size*\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show with beta that varies with sample size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-double selection solves this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-double selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& y_i = x_i' \\alpha_0  + z_i' \\beta_0 + \\varepsilon_i \\\\\n",
    "& x_i = z_i' \\gamma_0 + u_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We would like to guard against pretest bias if possible, in order to handle high dimensional models. A good pathway towards motivating procedures which guard against pretest bias is a discussion of classical partitioned regression.\n",
    "\n",
    "Consider a regression $y_i$ on $x_i$ and $z_i$. $x_i$ is the 1-dimensional variable of interest, $z_i$ is a high-dimensional set of control variables. We have the following procedure:\n",
    "\n",
    "1. **First Stage** selection: lasso $x_i$ on $z_i$. Let the selected variables be collected in the set $S_{FS} \\subseteq z_i$\n",
    "2. **Reduced Form** selection: lasso $y_i$ on $z_i$. Let the selected variables be collected in the set $S_{RF} \\subseteq z_i$\n",
    "3. Regress $y_i$ on $x_i$ and $S_{FS} \\cup S_{RF}$\n",
    "\n",
    "\n",
    "**Theorem**:\n",
    "Let $\\{P^n\\}$ be a sequence of data-generating processes for $D_n = (y_i, x_i, z_i)^n_{i=1} \\in (\\mathbb R \\times \\mathbb R \\times \\mathbb R^p) ^n$ where $p$ depends on $n$. For each $n$, the data are iid with $yi =  x_i'\\alpha_0^{(n)} + z_i' \\beta_0^{(n)} + \\varepsilon_i$ and $x_i = z_i' \\gamma_0^{(n)} + u_i$ where $\\mathbb E[\\varepsilon_i | x_i,z_i] = 0$ and $\\mathbb E[u_i|z_i] = 0$. The sparsity of the vectors  $\\beta_0^{(n)}$, $\\gamma_0^{(n)}$ is controlled by $|| \\beta_0^{(n)} ||_0 \\leq s$ with $s^2 (\\log p)^2/n \\to 0$. Suppose that additional regularity conditions on the model selection procedures and moments of the random variables $y_i$ , $x_i$ , $z_i$ as documented in Belloni et al. (2014). Then the confidence intervals, CI, from the post double selection procedure are uniformly valid. That is, for any confidence level $\\xi \\in (0, 1)$\n",
    "$$\n",
    "\t\t\\Pr(\\alpha_0 \\in CI) \\to 1- \\xi\n",
    "$$\n",
    "\n",
    "In order to have valid confidence intervals you want their bias to be negligibly. Since\n",
    "$$\n",
    "  CI = \\left[ \\hat{\\alpha} \\pm \\frac{1.96 \\cdot \\hat{\\sigma}}{\\sqrt{n}} \\right]\n",
    "$$\n",
    "\n",
    "If the bias is $o \\left( \\frac{1}{\\sqrt{n}} \\right)$ then there is no problem since it is asymptotically negligible w.r.t. the magnitude of the confidence interval. If however the the bias is $O \\left( \\frac{1}{\\sqrt{n}} \\right)$ then it has the same magnitude of the confidence interval and it does not asymptotically vanish. \n",
    "\n",
    "The idea of the proof is to use partitioned regression. An alternative way to think about the argument is: bound the omitted variables bias. Omitted variable bias comes from the product of 2 quantities related to the omitted variable:\n",
    "\n",
    "1. Its partial correlation with the outcome, and\n",
    "2. Its partial correlation with the variable of interest.\n",
    "\n",
    "If both those partial correlations are $O( \\sqrt{\\log p/n})$, then the omitted variables bias is $(s \\times O( \\sqrt{\\log p/n})^2 = o \\left( \\frac{1}{\\sqrt{n}} \\right)$, provided $s^2 (\\log p)^2/n \\to 0$. Relative to the $ \\frac{1}{\\sqrt{n}} $ convergence rate, the omitted variables bias is negligible. \n",
    "\n",
    "In our omitted variable bias case, we want $| \\beta_0 \\gamma_0 | = o \\left( \\frac{1}{\\sqrt{n}} \\right)$.  Post-double selection guarantees that \n",
    "\n",
    "- *Reduced form* selection (pre-testing): any \"missing\" variable has $|\\beta_{0j}| \\leq \\frac{c}{\\sqrt{n}}$\n",
    "- *First stage* selection (additional): any \"missing\" variable has $|\\gamma_{0j}| \\leq \\frac{c}{\\sqrt{n}}$\n",
    "\n",
    "\n",
    "As a consequence, as long as the number of omitted variables is finite, the omitted variable bias is \n",
    "$$\n",
    "\tOVB(\\alpha) = |\\beta_{0j}| \\cdot|\\gamma_{0j}| \\leq \\frac{c}{\\sqrt{n}} \\cdot \\frac{c}{\\sqrt{n}} = \\frac{c^2}{n} = o \\left(\\frac{1}{\\sqrt{n}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application to Effect of Legalized Abortion on Crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double/debiased machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is taken from [Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). \"*Double/debiased machine learning for treatment and structural parameters*\"](https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following partially linear model\n",
    "\n",
    "$$\n",
    "y = \\beta_0 D + g_0(X) + \\varepsilon \\\\\n",
    "D = m_0(X) + u\n",
    "$$\n",
    "\n",
    "where $y$ is the outcome variable, $D$ is the treatment to interest and $X$ is a potentially high-dimensional set of controls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive approach\n",
    "\n",
    "A naive approach to estimation of $\\beta_0$ using ML methods would be, for example, to construct a sophisticated ML estimator $\\beta_0 D + g_0(X)$ for learning the regression function $\\beta_0 D$ + $g_0(X)$.\n",
    "\n",
    "1. Split the sample in two: main sample and auxiliary sample\n",
    "2. Use the auxiliary sample to estimate $\\hat g_0(X)$\n",
    "3. Use the main sample to estimate the residualized OLS estimator\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{0}=\\left(\\frac{1}{n} \\sum_{i \\in I} D_{i}^{2}\\right)^{-1} \\frac{1}{n} \\sum_{i \\in I} D_{i}\\left(Y_{i}-\\hat{g}_{0}\\left(X_{i}\\right)\\right)\n",
    "$$\n",
    "\n",
    "This estimator is going to have two problems:\n",
    "\n",
    "1. Slow rate of convergence, i.e. slower than $n^{1/2}$\n",
    "2. It will be biased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orthogonalization\n",
    "\n",
    "Now consider a second construction that employs an orthogonalized formulation obtained by directly partialling out the effect of $X$ from $D$ to obtain the orthogonalized regressor $V = D âˆ’ m_0(X)$.\n",
    "\n",
    "1. Split the sample in two: main sample and auxiliary sample\n",
    "2. Use the auxiliary sample to estimate $\\hat g_0(X)$\n",
    "3. Use the auxiliary sample to estimate $\\hat m_0(X)$\n",
    "4. Build the orthogonalized component of $X$: $\\hat u = D - \\hat m_0(X)$\n",
    "5. Use the main sample to estimate the double-residualized OLS estimator\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{0}=\\left(\\frac{1}{n} \\sum_{i \\in I} \\hat u_i D_{i} \\right)^{-1} \\frac{1}{n} \\sum_{i \\in I} \\hat u_i \\left(Y_{i}-\\hat{g}_{0}\\left(X_{i}\\right)\\right)\n",
    "$$\n",
    "\n",
    "The estimator is unbiased but still has a lower rate of convergence because of sample splitting. The proble is solved by inverting the split sample, re-estimating the coefficient and averaging the two estimates. Note that this procedure is valid since the two estimates are independent by the sample splitting procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application to AJR02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to replicate 6.3 of the \"*Double/debiased machine learning*\" paper based on [Acemoglu, Johnson, Robinson (2002), \"*The Colonial Origins of Comparative Development*\"](https://economics.mit.edu/files/4123).\n",
    "\n",
    "We first load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Acemoglu Johnson Robinson Dataset\n",
    "df = pd.read_csv('data/AJR02.csv')\n",
    "df = df.loc[df['baseco']==1,:]\n",
    "df = df[['avexpr','logpgp95', 'lat_abst', 'asia', 'africa', 'other','logem4']]\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>avexpr</th>\n",
       "      <th>logpgp95</th>\n",
       "      <th>lat_abst</th>\n",
       "      <th>asia</th>\n",
       "      <th>africa</th>\n",
       "      <th>other</th>\n",
       "      <th>logem4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.363637</td>\n",
       "      <td>7.770645</td>\n",
       "      <td>0.136667</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.634790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>6.386363</td>\n",
       "      <td>9.133459</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.232656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>9.318182</td>\n",
       "      <td>9.897972</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.145931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>4.454546</td>\n",
       "      <td>6.845880</td>\n",
       "      <td>0.144444</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.634790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>5.136363</td>\n",
       "      <td>6.877296</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.268438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    avexpr  logpgp95  lat_abst  asia  africa  other    logem4\n",
       "0      1  5.363637  7.770645  0.136667     0       1      0  5.634790\n",
       "1      3  6.386363  9.133459  0.377778     0       0      0  4.232656\n",
       "2      5  9.318182  9.897972  0.300000     0       0      1  2.145931\n",
       "3     11  4.454546  6.845880  0.144444     0       1      0  5.634790\n",
       "4     12  5.136363  6.877296  0.266667     1       0      0  4.268438"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64 entries, 0 to 63\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   index     64 non-null     int64  \n",
      " 1   avexpr    64 non-null     float64\n",
      " 2   logpgp95  64 non-null     float64\n",
      " 3   lat_abst  64 non-null     float64\n",
      " 4   asia      64 non-null     int64  \n",
      " 5   africa    64 non-null     int64  \n",
      " 6   other     64 non-null     int64  \n",
      " 7   logem4    64 non-null     float64\n",
      "dtypes: float64(4), int64(4)\n",
      "memory usage: 4.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their paper, AJR note that their IV strategy will be invalidated if other factors are also highly persistent and related to the development of institutions within a country and to the countryâ€™s GDP. A leading candidate for such a factor, as they discuss, is geography. AJR address this by assuming that the confounding effect of geography is adequately captured by a linear term in distance from the equator and a set of continent dummy variables. \n",
    "\n",
    "They inclue their results in table 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add constant term to dataset\n",
    "df['const'] = 1\n",
    "\n",
    "# Create lists of variables to be used in each regression\n",
    "X1 = df[['const', 'avexpr']]\n",
    "X2 = df[['const', 'avexpr', 'lat_abst']]\n",
    "X3 = df[['const', 'avexpr', 'lat_abst', 'asia', 'africa', 'other']]\n",
    "y = df['logpgp95']\n",
    "\n",
    "# Estimate an OLS regression for each set of variables\n",
    "reg1 = sm.OLS(y, X1, missing='drop').fit()\n",
    "reg2 = sm.OLS(y, X2, missing='drop').fit()\n",
    "reg3 = sm.OLS(y, X3, missing='drop').fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>         <th>Model 1</th> <th>Model 3</th>  <th>Model 4</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>4.66***</td> <td>4.73***</td>  <td>5.74***</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th></th>                 <td>(0.41)</td>  <td>(0.40)</td>   <td>(0.40)</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>avexpr</th>           <td>0.52***</td> <td>0.47***</td>  <td>0.40***</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th></th>                 <td>(0.06)</td>  <td>(0.06)</td>   <td>(0.06)</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lat_abst</th>            <td></td>     <td>1.58**</td>    <td>0.88</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th></th>                    <td></td>     <td>(0.71)</td>   <td>(0.63)</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>asia</th>                <td></td>        <td></td>      <td>-0.58**</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th></th>                    <td></td>        <td></td>      <td>(0.23)</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>africa</th>              <td></td>        <td></td>     <td>-0.88***</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th></th>                    <td></td>        <td></td>      <td>(0.17)</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>other</th>               <td></td>        <td></td>       <td>0.11</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th></th>                    <td></td>        <td></td>      <td>(0.38)</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>R-squared</th>         <td>0.54</td>    <td>0.57</td>     <td>0.71</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>R-squared Adj.</th>    <td>0.53</td>    <td>0.56</td>     <td>0.69</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. observations</th>   <td>64</td>      <td>64</td>       <td>64</td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "\n",
       "=========================================\n",
       "                 Model 1 Model 3 Model 4 \n",
       "-----------------------------------------\n",
       "const            4.66*** 4.73*** 5.74*** \n",
       "                 (0.41)  (0.40)  (0.40)  \n",
       "avexpr           0.52*** 0.47*** 0.40*** \n",
       "                 (0.06)  (0.06)  (0.06)  \n",
       "lat_abst                 1.58**  0.88    \n",
       "                         (0.71)  (0.63)  \n",
       "asia                             -0.58** \n",
       "                                 (0.23)  \n",
       "africa                           -0.88***\n",
       "                                 (0.17)  \n",
       "other                            0.11    \n",
       "                                 (0.38)  \n",
       "R-squared        0.54    0.57    0.71    \n",
       "R-squared Adj.   0.53    0.56    0.69    \n",
       "No. observations 64      64      64      \n",
       "=========================================\n",
       "Standard errors in parentheses.\n",
       "* p<.1, ** p<.05, ***p<.01\n",
       "\"\"\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_dict={'No. observations' : lambda x: f\"{int(x.nobs):d}\"}\n",
    "\n",
    "results_table = summary_col(results=[reg1,reg2,reg3],\n",
    "                            float_format='%0.2f',\n",
    "                            stars = True,\n",
    "                            model_names=['Model 1','Model 3','Model 4'],\n",
    "                            info_dict=info_dict,\n",
    "                            regressor_order=['const','avexpr','lat_abst','asia','africa'])\n",
    "\n",
    "results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using DML allows us to relax this assumption and to replace it by a weaker assumption that geography can be sufficiently controlled by an unknown function of distance from the equator and continent dummies, which can be learned by ML methods.\n",
    "\n",
    "In particular, our framework is\n",
    "\n",
    "$$\n",
    "{logpgp95} = \\beta_0 \\times {avexpr} + g_0({geography}) + \\varepsilon \\\\\n",
    "{avexpr} = m_0({geography}) + u\n",
    "$$\n",
    "\n",
    "So that the double/debiased machine learning procedure is\n",
    "\n",
    "1. Split the sample in two: main sample and auxiliary sample\n",
    "2. Use the auxiliary sample to estimate $\\hat g_0({geography})$ \n",
    "3. Use the auxiliary sample to estimate $\\hat m_0({geography})$\n",
    "4. Build the orthogonalized component of ${geography}$: $\\hat u = {avexpr} - \\hat m_0({geography})$\n",
    "5. Use the main sample to estimate the double-residualized OLS estimator\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{0}=\\left(\\frac{1}{n} \\sum_{i \\in I} \\hat u_i \\times {avexpr}_{i} \\right)^{-1} \\frac{1}{n} \\sum_{i \\in I} \\hat u_i \\times \\left({logpgp95}_{i}-\\hat{g}_{0}\\left({geography}_{i}\\right)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate variables\n",
    "D = df['avexpr'].values.reshape(-1,1)\n",
    "X = df[['const', 'lat_abst', 'asia', 'africa', 'other']].values.reshape(-1,5)\n",
    "y = df['logpgp95'].values.reshape(-1,1)\n",
    "Z = df[['const', 'lat_abst', 'asia', 'africa', 'other','logem4']].values.reshape(-1,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write down the whole procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_beta(algorithm, alg_name, D, X, y, Z, size=0.5, verbose=False):\n",
    "    \n",
    "    # Split sample\n",
    "    D_main, D_aux, X_main, X_aux, y_main, y_aux, Z_main, Z_aux = train_test_split(D, X, y, Z, test_size=size)\n",
    "    \n",
    "    # Residualize y on D\n",
    "    y_resid = y_aux - D_aux @ inv(D_aux.T @ D_aux) @ D_aux.T @ y_aux\n",
    "    alg_fitted = algorithm.fit(X=X_aux, y=y_resid.ravel())\n",
    "    \n",
    "    # Estimate g0\n",
    "    g0 = alg_fitted.predict(X_main).reshape(-1,1)\n",
    "    alg_fitted = algorithm.fit(X=Z_aux, y=D_aux.ravel())\n",
    "    \n",
    "    # Estimate m0\n",
    "    m0 = algorithm.predict(Z_main).reshape(-1,1)\n",
    "    \n",
    "    # Build u_hat\n",
    "    u_hat = D_main - m0\n",
    "    \n",
    "    # Estimate beta\n",
    "    beta = inv(u_hat.T @ D_main) @ u_hat.T @ (y_main - g0_tree)\n",
    "    if verbose:\n",
    "        print('%s : %.4f' % (alg_name, beta[0][0]))\n",
    "    return beta[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now repeat the same process with different algorithms. In particular, we consider:\n",
    "\n",
    "1. Lasso\n",
    "2. Regression Trees\n",
    "3. Random Forest\n",
    "4. Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all algorithms\n",
    "algorithms = {'Lasso   ': Lasso(), \n",
    "              'Tree    ': DecisionTreeRegressor(), \n",
    "              'Forest  ': RandomForestRegressor(),\n",
    "              'Boosting': GradientBoostingRegressor()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso    : 0.9438\n",
      "Tree     : 0.6737\n",
      "Forest   : 0.7397\n",
      "Boosting : 1.0306\n"
     ]
    }
   ],
   "source": [
    "# Loop over algorithms\n",
    "for alg_name, algorithm in algorithms.items():\n",
    "    estimate_beta(algorithm, alg_name, D, X, y, Z, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat 100 times\n",
    "def estimate_beta_n(algorithms, n, D, X, y, Z):\n",
    "    \n",
    "    # Init\n",
    "    betas = dict.fromkeys(algorithms.keys(), [])\n",
    "    \n",
    "    # Iterate n times\n",
    "    for i in range(n):\n",
    "    \n",
    "        # Loop over algorithms\n",
    "        for alg_name, algorithm in algoritms.items():\n",
    "            beta = estimate_beta(algorithm, alg_name, D, X, y, Z)\n",
    "            betas[alg_name] = np.append(betas[alg_name], beta)\n",
    "    \n",
    "    # Print coefficients and standard errors\n",
    "    for alg_name in algorithms.keys():\n",
    "        print('%s : %.4f (%.4f)' % (alg_name, np.mean(betas[alg_name]), np.std(betas[alg_name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso    : 0.3898 (1.2581)\n",
      "Tree     : 1.9663 (4.4479)\n",
      "Forest   : 6.8110 (12.8342)\n",
      "Boosting : -1.8580 (7.1255)\n"
     ]
    }
   ],
   "source": [
    "estimate_beta_n(algorithms, 10, D, X, y, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
